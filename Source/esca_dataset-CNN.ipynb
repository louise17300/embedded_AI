{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfwQTFZl9Gf7"
      },
      "source": [
        "# **Preprocessing Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMcgVaRpXC0g"
      },
      "source": [
        "**UNZIP DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2FZ7OoQAOcb",
        "outputId": "569fa454-4546-48bb-b468-9a7e03a85203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv1MNz0EA8uT"
      },
      "source": [
        "dataset_name = \"/content/drive/MyDrive/augmented_esca_dataset\"\n",
        "dataset_destination = \"/content/augmented_esca_dataset\"\n",
        "\n",
        "!unzip  $dataset_name\".zip\" -d $dataset_destination\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss4aXqoi9OaZ"
      },
      "source": [
        "**LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKV_DpMO9dI5"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5L_AeAq9JhF"
      },
      "source": [
        "**DIRECTORY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnNOs8QB9eek"
      },
      "source": [
        "# directory of dataset\n",
        "dir_original = \"/content/augmented_esca_dataset/content/esca_dataset/augmented_esca_dataset\"\n",
        "\n",
        "# name of new dataset\n",
        "dir_processed = \"/content/augmented_esca_dataset_splited\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL2JPD_S9QJr"
      },
      "source": [
        "**PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxtO-Z769gZe"
      },
      "source": [
        "# size of new images\n",
        "size = 1280, 720"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPd-i4dw9TbW"
      },
      "source": [
        "**EXTRACTION OF DATASET INFORMATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWYUjeDB9iEz",
        "outputId": "4dcb1e13-961c-45b4-fda6-beb5b28c7f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_dir = pathlib.Path(dir_original)\n",
        "\n",
        "set_samples = ['train', 'validation', 'test']\n",
        "print(\"set_samples: \", set_samples, \"\\n\")\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in sorted(data_dir.glob('*'))])\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "print(\"class: \", CLASS_NAMES, \"\\n\")\n",
        "\n",
        "N_IMAGES = np.array([len(list(data_dir.glob(item.name+'/*.jpg'))) for item in sorted(data_dir.glob('*'))])\t\t\t# number of images for class\n",
        "print(\"number of images for class: \", N_IMAGES, \"\\n\")\n",
        "\n",
        "N_samples = np.array([(int(np.around(n*60/100)), int(np.around(n*15/100)), int(np.around(n*25/100))) for n in N_IMAGES])\t# number of images for set (train,validation,test)\n",
        "print(\"split of dataset: \\n \", N_samples, \"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set_samples:  ['train', 'validation', 'test'] \n",
            "\n",
            "class:  ['esca' 'healthy'] \n",
            "\n",
            "number of images for class:  [12432 12348] \n",
            "\n",
            "split of dataset: \n",
            "  [[7459 1865 3108]\n",
            " [7409 1852 3087]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWj1AMlf_3LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmhOzI_Y9W8B"
      },
      "source": [
        "**PREPROCESSING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsf53Pzk7xgT"
      },
      "source": [
        "# Create the new dataset\n",
        "# Split Dataset\t\t\t\t\t\t\t\t(also resize and rotate)\n",
        "\n",
        "\n",
        "\n",
        "# create the dataset folder\t\t\t***********************************\n",
        "os.makedirs(dir_processed)\n",
        "\n",
        "for set_tag in set_samples:\n",
        "\tos.makedirs(dir_processed + '/' + set_tag)\n",
        "\n",
        "\tfor class_name in CLASS_NAMES:\n",
        "\t\tos.makedirs(dir_processed + '/' + set_tag + '/' + class_name)\n",
        "\n",
        "\n",
        "\n",
        "# SPLIT DATASET (and resize)\t\t*************************************\n",
        "print(\"Split dataset.....\")\n",
        "\n",
        "i=0\n",
        "j=0\n",
        "k=0\n",
        "for class_name in CLASS_NAMES:\t\t\t\t\t\t\t\t\t\t\t\t\t\t# \"j\" cambia con il tipo di pianta [0,3]\n",
        "\t\n",
        "    print(\"class name: \", class_name)\n",
        "\n",
        "    contatore_samples = 0\n",
        "    k=0\n",
        "\n",
        "    array = sorted(os.listdir(dir_original + '/' + class_name))\n",
        "    #random.shuffle(array)\n",
        "\n",
        "    for image_name in array:\t                                       \t# \"contatore\" si azzera ad ogni campo 'train' 'validation' 'test'\n",
        "\t\n",
        "        print(\"image: \", i)\n",
        "        i=i+1\n",
        "\n",
        "        if contatore_samples==N_samples[j][k]:\t\t\t\t\t\t\t\t\t\t    # \"k\" cambia con train, validation, e test\n",
        "            k+=1\n",
        "            contatore_samples=0\n",
        "\n",
        "\n",
        "        img=Image.open(dir_original +'/'+class_name+'/'+image_name)\n",
        "        l,_ = img.size\n",
        "        l=int(l)\n",
        "        \n",
        "        \n",
        "        if l==1080 or l==720:\n",
        "        \n",
        "            transposed = img.transpose(Image.ROTATE_90)\n",
        "            transposed.thumbnail(size)\n",
        "            transposed.save(dir_processed+'/'+set_samples[k]+'/'+class_name+'/'+image_name)\n",
        "        \n",
        "        else:\n",
        "        \n",
        "            img.thumbnail(size)\n",
        "            img.save(dir_processed+'/'+set_samples[k]+'/'+class_name+'/'+image_name)\n",
        "\n",
        "        contatore_samples+=1\t\n",
        "\n",
        "    j+=1\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ecfMI2lffI"
      },
      "source": [
        "# **MODEL for ESCA DATASET**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhxyVMvTlipG"
      },
      "source": [
        "LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X5lBUEJmN0k"
      },
      "source": [
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow import keras\n",
        " \n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        " \n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ituslInJllsc"
      },
      "source": [
        "DIRECTORY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFNFmre4mOig"
      },
      "source": [
        "# cartelle contenenti il dataset\n",
        " \n",
        "PATH_DATASET = '/content/augmented_esca_dataset_splited'\n",
        " \n",
        "train_data_dir = os.path.join(PATH_DATASET, 'train')\n",
        "validation_data_dir = os.path.join(PATH_DATASET, 'validation')\n",
        "test_data_dir = os.path.join(PATH_DATASET, 'test')\n",
        " \n",
        " \n",
        " \n",
        "# nomi dei file da creare\n",
        " \n",
        "PATH_MODELS = '/content/Colab Notebooks/PAPER_1'\n",
        " \n",
        "name_model_small = os.path.join(PATH_MODELS, 'model_small_b32.h5')\n",
        "name_model_medium = os.path.join(PATH_MODELS, 'model_medium_b32.h5')\n",
        "name_model_large = os.path.join(PATH_MODELS, 'model_large_b32.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0RNUATslnli"
      },
      "source": [
        "PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BhD4g_MmPMr"
      },
      "source": [
        "batch_size = 32\n",
        " \n",
        "nb_train_samples = 14868\n",
        "nb_validation_samples = 3717\n",
        "nb_test_samples = 6195\n",
        " \n",
        "n_class = 2\n",
        " \n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WQ9_a0bYAwq"
      },
      "source": [
        "# **MODEL LARGE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rlacu5qYBhn"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# image size (Model Medium)\n",
        "img_width, img_height = 1280, 720\n",
        "\n",
        "# input shape\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ************        DATASET       *************************************\n",
        "# ***********************************************************************\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_data_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=batch_size,\n",
        "                                             image_size=(img_width, img_height),\n",
        "                                             label_mode='categorical')\n",
        "\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_data_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  image_size=(img_width, img_height),\n",
        "                                                  label_mode='categorical')\n",
        "\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_data_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=batch_size,\n",
        "                                            image_size=(img_width, img_height),\n",
        "                                            label_mode='categorical')\n",
        "\n",
        "\n",
        "# preprocessing: input scaling (./255)\n",
        "train_dataset = train_dataset.map(lambda images, labels: (images/255, labels))\n",
        "validation_dataset = validation_dataset.map(lambda images, labels: (images/255, labels))\n",
        "test_dataset = test_dataset.map(lambda images, labels: (images/255, labels))\n",
        "\n",
        "\n",
        "# Configure the dataset for performance\n",
        "\n",
        "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# **************        MODEL       *************************************\n",
        "# ***********************************************************************\n",
        "\n",
        "model_large = Sequential()\n",
        "model_large.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_large.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_large.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_large.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_large.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_large.add(Flatten())\n",
        "model_large.add(Dense(64))\n",
        "model_large.add(Activation('relu'))\n",
        "model_large.add(Dropout(0.5))\n",
        "model_large.add(Dense(2))\t\t\t#because we have 2 class\n",
        "model_large.add(Activation('softmax'))\n",
        "\n",
        "model_large.summary()\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *******************        COMPILATION       **************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "model_large.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.Adadelta(learning_rate=1, name='Adadelta'),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *******************        TRAINING       *****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  history = model_large.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *****************        SAVE MODEL        ****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "model_large.save(name_model_large)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ********************        PLOT RESULTS        ***********************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy_'+str(img_width)+' x '+str(img_height))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss_'+str(img_width)+' x '+str(img_height))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ***********************        TEST        ****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  test_result = model_large.evaluate(test_dataset)\n",
        "\n",
        "  \n",
        "print(\"size of images: \", img_width,img_height)\n",
        "print(\"test_result: \", test_result)\n",
        "\n",
        "\n",
        "print ('Time taken for development model small {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsAyqNAKaFjJ"
      },
      "source": [
        "# **MODEL SMALL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSAhC_EFaHdD"
      },
      "source": [
        "start = time.time()\n",
        " \n",
        "# image size (Model Small)\n",
        "img_width, img_height = 80, 45\n",
        " \n",
        "# input shape\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# ************        DATASET       *************************************\n",
        "# ***********************************************************************\n",
        " \n",
        "train_dataset = image_dataset_from_directory(train_data_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=batch_size,\n",
        "                                             image_size=(img_width, img_height),\n",
        "                                             label_mode='categorical')\n",
        " \n",
        " \n",
        "validation_dataset = image_dataset_from_directory(validation_data_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  image_size=(img_width, img_height),\n",
        "                                                  label_mode='categorical')\n",
        " \n",
        " \n",
        "test_dataset = image_dataset_from_directory(test_data_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=batch_size,\n",
        "                                            image_size=(img_width, img_height),\n",
        "                                            label_mode='categorical')\n",
        " \n",
        " \n",
        "# preprocessing: input scaling (./255)\n",
        "train_dataset = train_dataset.map(lambda images, labels: (images/255, labels))\n",
        "validation_dataset = validation_dataset.map(lambda images, labels: (images/255, labels))\n",
        "test_dataset = test_dataset.map(lambda images, labels: (images/255, labels))\n",
        " \n",
        " \n",
        "# Configure the dataset for performance\n",
        " \n",
        "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        " \n",
        "#train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# **************        MODEL       *************************************\n",
        "# ***********************************************************************\n",
        " \n",
        "model_small = Sequential()\n",
        "model_small.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "model_small.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "model_small.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "model_small.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "model_small.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "model_small.add(Flatten())\n",
        "model_small.add(Dense(64))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(Dropout(0.5))\n",
        "model_small.add(Dense(2))           #because we have 2 class\n",
        "model_small.add(Activation('softmax'))\n",
        " \n",
        "model_small.summary()\n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# *******************        COMPILATION       **************************\n",
        "# ***********************************************************************\n",
        " \n",
        " \n",
        "model_small.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.Adadelta(learning_rate=1, name='Adadelta'),\n",
        "            metrics=['accuracy'])\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# *******************        TRAINING       *****************************\n",
        "# ***********************************************************************\n",
        " \n",
        " \n",
        "with tf.device('/device:GPU:0'):\n",
        " \n",
        "  history = model_small.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset)\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# *****************        SAVE MODEL        ****************************\n",
        "# ***********************************************************************\n",
        " \n",
        " \n",
        "model_small.save(name_model_small)\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# ********************        PLOT RESULTS        ***********************\n",
        "# ***********************************************************************\n",
        " \n",
        " \n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        " \n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        "epochs_range = range(epochs)\n",
        " \n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy_'+str(img_width)+' x '+str(img_height))\n",
        " \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss_'+str(img_width)+' x '+str(img_height))\n",
        "plt.show()\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# ***********************        TEST        ****************************\n",
        "# ***********************************************************************\n",
        " \n",
        "with tf.device('/device:GPU:0'):\n",
        " \n",
        "  test_result = model_small.evaluate(test_dataset)\n",
        " \n",
        "  \n",
        "print(\"size of images: \", img_width,img_height)\n",
        "print(\"test_result: \", test_result)\n",
        " \n",
        " \n",
        "print ('Time taken for development model small {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On extrait les datasets en fichier .npy"
      ],
      "metadata": {
        "id": "OPSK5KIuDKVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        " \n",
        "# image size (Model Small)\n",
        "img_width, img_height = 80, 45\n",
        " \n",
        "# input shape\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        " \n",
        " \n",
        " \n",
        "# ***********************************************************************\n",
        "# ************        DATASET       *************************************\n",
        "# ***********************************************************************\n",
        " \n",
        "train_dataset = image_dataset_from_directory(train_data_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=batch_size,\n",
        "                                             image_size=(img_width, img_height),\n",
        "                                             label_mode='categorical')\n",
        " \n",
        " \n",
        "validation_dataset = image_dataset_from_directory(validation_data_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  image_size=(img_width, img_height),\n",
        "                                                  label_mode='categorical')\n",
        " \n",
        " \n",
        "test_dataset = image_dataset_from_directory(test_data_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=batch_size,\n",
        "                                            image_size=(img_width, img_height),\n",
        "                                            label_mode='categorical')\n",
        " \n",
        "\n",
        "for images, labels in validation_dataset:  # only take first element of dataset\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "\n",
        "np.save(\"validation_x_set\", numpy_images)\n",
        "np.save(\"validation_y_set\", numpy_labels)"
      ],
      "metadata": {
        "id": "UELiVIsyDJ_2",
        "outputId": "be2b68b1-f4fe-4b7f-8445-4c736fca599b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14868 files belonging to 2 classes.\n",
            "Found 3717 files belonging to 2 classes.\n",
            "Found 6195 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp \"/content/validation_x_set.npy\" \"/content/drive/My Drive/validation_x_set.npy\"\n",
        "!cp \"/content/validation_y_set.npy\" \"/content/drive/My Drive/validation_y_set.npy\""
      ],
      "metadata": {
        "id": "x01_N-l3S5J_",
        "outputId": "a47b0d37-924e-48e4-f8f8-44b494e3b2c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK6gqFKXWtUv"
      },
      "source": [
        "# **MODEL MEDIUM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xArrgnbWt3y"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# image size (Model Medium)\n",
        "img_width, img_height = 320, 180\n",
        "\n",
        "# input shape\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ************        DATASET       *************************************\n",
        "# ***********************************************************************\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_data_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=batch_size,\n",
        "                                             image_size=(img_width, img_height),\n",
        "                                             label_mode='categorical')\n",
        "\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_data_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  image_size=(img_width, img_height),\n",
        "                                                  label_mode='categorical')\n",
        "\n",
        "\n",
        "test_dataset = image_dataset_from_directory(test_data_dir,\n",
        "                                            shuffle=True,\n",
        "                                            batch_size=batch_size,\n",
        "                                            image_size=(img_width, img_height),\n",
        "                                            label_mode='categorical')\n",
        "\n",
        "\n",
        "# preprocessing: input scaling (./255)\n",
        "train_dataset = train_dataset.map(lambda images, labels: (images/255, labels))\n",
        "validation_dataset = validation_dataset.map(lambda images, labels: (images/255, labels))\n",
        "test_dataset = test_dataset.map(lambda images, labels: (images/255, labels))\n",
        "\n",
        "\n",
        "# Configure the dataset for performance\n",
        "\n",
        "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "#test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# **************        MODEL       *************************************\n",
        "# ***********************************************************************\n",
        "\n",
        "model_medium = Sequential()\n",
        "model_medium.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_medium.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_medium.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_medium.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_medium.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model_medium.add(Flatten())\n",
        "model_medium.add(Dense(64))\n",
        "model_medium.add(Activation('relu'))\n",
        "model_medium.add(Dropout(0.5))\n",
        "model_medium.add(Dense(2))\t\t\t#because we have 2 class\n",
        "model_medium.add(Activation('softmax'))\n",
        "\n",
        "model_medium.summary()\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *******************        COMPILATION       **************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "model_medium.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.Adadelta(learning_rate=1, name='Adadelta'),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *******************        TRAINING       *****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  history = model_medium.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# *****************        SAVE MODEL        ****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "model_medium.save(name_model_medium)\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ********************        PLOT RESULTS        ***********************\n",
        "# ***********************************************************************\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy_'+str(img_width)+' x '+str(img_height))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss_'+str(img_width)+' x '+str(img_height))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ***********************************************************************\n",
        "# ***********************        TEST        ****************************\n",
        "# ***********************************************************************\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  test_result = model_medium.evaluate(test_dataset)\n",
        "\n",
        "  \n",
        "print(\"size of images: \", img_width,img_height)\n",
        "print(\"test_result: \", test_result)\n",
        "\n",
        "\n",
        "print ('Time taken for development model small {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}